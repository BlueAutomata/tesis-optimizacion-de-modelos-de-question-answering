{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluaci√≥n de Modelos con Conjunto de Datos Propio",
   "id": "7546131f55ea50f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 1: Cargar las Librer√≠as",
   "id": "924f60ad5880f533"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-29T05:04:36.368510Z",
     "start_time": "2025-10-29T05:04:36.363491Z"
    }
   },
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 2: Cargar los modelos",
   "id": "d1cc51d9fc9cf266"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T05:04:36.386027Z",
     "start_time": "2025-10-29T05:04:36.381515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = {\n",
    "    \"BERT (Base)\": \"mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\",\n",
    "    \"DistilBERT (Base)\": \"mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\",\n",
    "    \"BERT (Fine-tuned News QA)\": \"BlueAutomata/bert-base-spanish-wwm-cased-news-qa-colombia-mexico\",\n",
    "    \"DistilBERT (Fine-tuned News QA)\": \"BlueAutomata/distill-bert-base-spanish-wwm-cased-news-qa-colombia-mexico\",\n",
    "}"
   ],
   "id": "af705ace229652e0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 3: Cargar los conjuntos de datos",
   "id": "97b75fc8fc1794a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T05:04:36.399490Z",
     "start_time": "2025-10-29T05:04:36.395543Z"
    }
   },
   "cell_type": "code",
   "source": "GOLD_PATH   = \"../datasets/exploration_datasets/gold/\"",
   "id": "e9187b9ad8f066f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T05:04:36.409924Z",
     "start_time": "2025-10-29T05:04:36.405499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets = {\n",
    "    \"Colombia and Mexico Train\": GOLD_PATH + \"train_colombia_mexico_dataset.json\",\n",
    "    \"Colombia and Mexico Eval\": GOLD_PATH + \"eval_colombia_mexico_dataset.json\",\n",
    "    \"Colombia Train\": GOLD_PATH + \"train_colombia_dataset.json\",\n",
    "    \"Mexico Train\": GOLD_PATH + \"train_mexico_dataset.json\",\n",
    "    \"Colombia Eval\": GOLD_PATH + \"eval_colombia_dataset.json\",\n",
    "    \"Mexico Eval\": GOLD_PATH + \"eval_mexico_dataset.json\"\n",
    "}"
   ],
   "id": "413cfd734b91e4fe",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 4: Cargar las m√©tricas",
   "id": "e3269e9e7214f823"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T05:04:38.494268Z",
     "start_time": "2025-10-29T05:04:36.417442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# üßÆ Metric (SQuAD v2)\n",
    "metric = evaluate.load(\"squad_v2\")"
   ],
   "id": "c59f411fc62d6d99",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 5: Transformar el conjunto de datos a espa√±ol: Aplanar a formato Contexto + Preguntas y Respuestas (QA",
   "id": "676fd05ce3d9a9db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T05:04:38.510366Z",
     "start_time": "2025-10-29T05:04:38.504781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flatten_squad(dataset):\n",
    "    # Handle \"data\" key if present\n",
    "    if isinstance(dataset, dict) and \"data\" in dataset:\n",
    "        dataset = dataset[\"data\"]\n",
    "\n",
    "    new_data = []\n",
    "    for article in dataset:\n",
    "        for para in article[\"paragraphs\"]:\n",
    "            new_data.append({\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"context\": para[\"context\"],\n",
    "                \"qas\": para[\"qas\"]\n",
    "            })\n",
    "    return new_data"
   ],
   "id": "d9758adff8db89ff",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 5: Evaluar el Modelo",
   "id": "6f681c9e6bf3878"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T05:04:38.531764Z",
     "start_time": "2025-10-29T05:04:38.522884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model_name, model_path, dataset_dict):\n",
    "    print(f\"\\nüîπ Evaluating {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    for ds_name, ds_path in dataset_dict.items():\n",
    "        print(f\"\\nüìò Dataset: {ds_name}\")\n",
    "        dataset = json.load(open(ds_path, \"r\", encoding=\"utf-8\"))\n",
    "        flat_eval = flatten_squad(dataset)\n",
    "\n",
    "        predictions, references = [], []\n",
    "\n",
    "        for ex in tqdm(flat_eval):\n",
    "            for qa in ex[\"qas\"]:\n",
    "                if qa.get(\"is_impossible\", False):\n",
    "                    predictions.append({\n",
    "                        \"id\": qa[\"id\"],\n",
    "                        \"prediction_text\": \"\",\n",
    "                        \"no_answer_probability\": 1.0\n",
    "                    })\n",
    "                    references.append({\n",
    "                        \"id\": qa[\"id\"],\n",
    "                        \"answers\": {\"text\": [], \"answer_start\": []}\n",
    "                    })\n",
    "                else:\n",
    "                    try:\n",
    "                        pred = qa_pipeline(\n",
    "                            question=qa[\"question\"],\n",
    "                            context=ex[\"context\"],\n",
    "                            handle_impossible_answer=True\n",
    "                        )\n",
    "                        predictions.append({\n",
    "                            \"id\": qa[\"id\"],\n",
    "                            \"prediction_text\": pred[\"answer\"],\n",
    "                            \"no_answer_probability\": 1.0 - pred.get(\"score\", 0.0)\n",
    "                        })\n",
    "                        references.append({\n",
    "                            \"id\": qa[\"id\"],\n",
    "                            \"answers\": {\n",
    "                                \"text\": [a[\"text\"] for a in qa[\"answers\"]],\n",
    "                                \"answer_start\": [a[\"answer_start\"] for a in qa[\"answers\"]]\n",
    "                            }\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Skipped QA {qa.get('id', 'unknown')}: {e}\")\n",
    "\n",
    "        results = metric.compute(predictions=predictions, references=references)\n",
    "        print(f\"\\n‚úÖ Results for {ds_name}:\")\n",
    "        print(results)  # üëà useful to confirm the keys\n",
    "        print(f\"   ‚Ä¢ Exact Match (EM): {results.get('exact_match', results.get('exact', 0)):.2f}\")\n",
    "        print(f\"   ‚Ä¢ F1 Score: {results.get('f1', 0):.2f}\")"
   ],
   "id": "10cd45a53ce82b8d",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Ejecutar para ambos modelos",
   "id": "7a9b797da0eee633"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T06:45:17.883016Z",
     "start_time": "2025-10-29T05:04:38.546775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# üöÄ Run for both models\n",
    "for model_name, model_path in models.items():\n",
    "    evaluate_model(model_name, model_path, datasets)"
   ],
   "id": "6e963dac81c4e2f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Evaluating BERT (Base)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò Dataset: Colombia and Mexico Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3235/3235 [2:19:24<00:00,  2.59s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia and Mexico Train:\n",
      "{'exact': 71.22425237801957, 'f1': 81.26662254248514, 'total': 14613, 'HasAns_exact': 58.60813072152771, 'HasAns_f1': 73.05336698625419, 'HasAns_total': 10159, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 4454, 'best_exact': 71.22425237801957, 'best_exact_thresh': 0.9643369317054749, 'best_f1': 81.26662254248333, 'best_f1_thresh': 0.9983832836151123}\n",
      "   ‚Ä¢ Exact Match (EM): 71.22\n",
      "   ‚Ä¢ F1 Score: 81.27\n",
      "\n",
      "üìò Dataset: Colombia and Mexico Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2845/2845 [1:01:13<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia and Mexico Eval:\n",
      "{'exact': 69.34376496886476, 'f1': 80.11290507369172, 'total': 6263, 'HasAns_exact': 56.09421449805625, 'HasAns_f1': 71.51775085216778, 'HasAns_total': 4373, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 1890, 'best_exact': 69.34376496886476, 'best_exact_thresh': 0.9807323813438416, 'best_f1': 80.11290507369165, 'best_f1_thresh': 0.994681179523468}\n",
      "   ‚Ä¢ Exact Match (EM): 69.34\n",
      "   ‚Ä¢ F1 Score: 80.11\n",
      "\n",
      "üìò Dataset: Colombia Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2564/2564 [1:30:31<00:00,  2.12s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia Train:\n",
      "{'exact': 71.48709154996666, 'f1': 81.22203178633036, 'total': 10497, 'HasAns_exact': 58.75137816979052, 'HasAns_f1': 72.83457382319534, 'HasAns_total': 7256, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 3241, 'best_exact': 71.48709154996666, 'best_exact_thresh': 0.9643369317054749, 'best_f1': 81.22203178633085, 'best_f1_thresh': 0.9956186413764954}\n",
      "   ‚Ä¢ Exact Match (EM): 71.49\n",
      "   ‚Ä¢ F1 Score: 81.22\n",
      "\n",
      "üìò Dataset: Mexico Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1006/1006 [49:56<00:00,  2.98s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Mexico Train:\n",
      "{'exact': 70.5539358600583, 'f1': 81.3803419709045, 'total': 4116, 'HasAns_exact': 58.25008611780916, 'HasAns_f1': 73.60023684197158, 'HasAns_total': 2903, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 1213, 'best_exact': 70.5539358600583, 'best_exact_thresh': 0.9643369317054749, 'best_f1': 81.38034197090413, 'best_f1_thresh': 0.9983832836151123}\n",
      "   ‚Ä¢ Exact Match (EM): 70.55\n",
      "   ‚Ä¢ F1 Score: 81.38\n",
      "\n",
      "üìò Dataset: Colombia Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2211/2211 [38:56<00:00,  1.06s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia Eval:\n",
      "{'exact': 69.6599244276506, 'f1': 80.0648323705853, 'total': 4499, 'HasAns_exact': 56.68041891463027, 'HasAns_f1': 71.53655374016638, 'HasAns_total': 3151, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 1348, 'best_exact': 69.6599244276506, 'best_exact_thresh': 0.968170166015625, 'best_f1': 80.06483237058498, 'best_f1_thresh': 0.9943676590919495}\n",
      "   ‚Ä¢ Exact Match (EM): 69.66\n",
      "   ‚Ä¢ F1 Score: 80.06\n",
      "\n",
      "üìò Dataset: Mexico Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 880/880 [20:37<00:00,  1.41s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Mexico Eval:\n",
      "{'exact': 68.5374149659864, 'f1': 80.23551226829183, 'total': 1764, 'HasAns_exact': 54.58265139116203, 'HasAns_f1': 71.46926648221498, 'HasAns_total': 1222, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 542, 'best_exact': 68.5374149659864, 'best_exact_thresh': 0.9807323813438416, 'best_f1': 80.23551226829194, 'best_f1_thresh': 0.994681179523468}\n",
      "   ‚Ä¢ Exact Match (EM): 68.54\n",
      "   ‚Ä¢ F1 Score: 80.24\n",
      "\n",
      "üîπ Evaluating DistilBERT (Base)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò Dataset: Colombia and Mexico Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3235/3235 [2:20:04<00:00,  2.60s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia and Mexico Train:\n",
      "{'exact': 69.63662492301376, 'f1': 80.65339573836589, 'total': 14613, 'HasAns_exact': 56.32444138202579, 'HasAns_f1': 72.1712837803682, 'HasAns_total': 10159, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 4454, 'best_exact': 69.63662492301376, 'best_exact_thresh': 0.9932949542999268, 'best_f1': 80.65339573836398, 'best_f1_thresh': 0.9993553161621094}\n",
      "   ‚Ä¢ Exact Match (EM): 69.64\n",
      "   ‚Ä¢ F1 Score: 80.65\n",
      "\n",
      "üìò Dataset: Colombia and Mexico Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2845/2845 [1:04:16<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia and Mexico Eval:\n",
      "{'exact': 68.51349193677152, 'f1': 80.1113882073937, 'total': 6263, 'HasAns_exact': 54.90509947404528, 'HasAns_f1': 71.51557839993242, 'HasAns_total': 4373, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 1890, 'best_exact': 68.51349193677152, 'best_exact_thresh': 0.9864137172698975, 'best_f1': 80.11138820739401, 'best_f1_thresh': 0.9993077516555786}\n",
      "   ‚Ä¢ Exact Match (EM): 68.51\n",
      "   ‚Ä¢ F1 Score: 80.11\n",
      "\n",
      "üìò Dataset: Colombia Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2564/2564 [1:33:15<00:00,  2.18s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia Train:\n",
      "{'exact': 70.02000571591883, 'f1': 80.49021422410055, 'total': 10497, 'HasAns_exact': 56.628996692392505, 'HasAns_f1': 71.77587909459476, 'HasAns_total': 7256, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 3241, 'best_exact': 70.02000571591883, 'best_exact_thresh': 0.9932949542999268, 'best_f1': 80.49021422410104, 'best_f1_thresh': 0.9992630481719971}\n",
      "   ‚Ä¢ Exact Match (EM): 70.02\n",
      "   ‚Ä¢ F1 Score: 80.49\n",
      "\n",
      "üìò Dataset: Mexico Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1006/1006 [50:13<00:00,  3.00s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Mexico Train:\n",
      "{'exact': 68.65889212827989, 'f1': 81.06955617453178, 'total': 4116, 'HasAns_exact': 55.56321047192559, 'HasAns_f1': 73.15959118648775, 'HasAns_total': 2903, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 1213, 'best_exact': 68.65889212827989, 'best_exact_thresh': 0.9733725786209106, 'best_f1': 81.06955617453147, 'best_f1_thresh': 0.9993553161621094}\n",
      "   ‚Ä¢ Exact Match (EM): 68.66\n",
      "   ‚Ä¢ F1 Score: 81.07\n",
      "\n",
      "üìò Dataset: Colombia Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2211/2211 [39:32<00:00,  1.07s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia Eval:\n",
      "{'exact': 68.90420093354079, 'f1': 80.20775625268904, 'total': 4499, 'HasAns_exact': 55.60139638210092, 'HasAns_f1': 71.74062055882223, 'HasAns_total': 3151, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 1348, 'best_exact': 68.90420093354079, 'best_exact_thresh': 0.971991777420044, 'best_f1': 80.20775625268875, 'best_f1_thresh': 0.9993077516555786}\n",
      "   ‚Ä¢ Exact Match (EM): 68.90\n",
      "   ‚Ä¢ F1 Score: 80.21\n",
      "\n",
      "üìò Dataset: Mexico Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 880/880 [21:25<00:00,  1.46s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Mexico Eval:\n",
      "{'exact': 67.51700680272108, 'f1': 79.86560598756057, 'total': 1764, 'HasAns_exact': 53.10965630114566, 'HasAns_f1': 70.93529374963727, 'HasAns_total': 1222, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 542, 'best_exact': 67.51700680272108, 'best_exact_thresh': 0.9864137172698975, 'best_f1': 79.86560598756068, 'best_f1_thresh': 0.9991227984428406}\n",
      "   ‚Ä¢ Exact Match (EM): 67.52\n",
      "   ‚Ä¢ F1 Score: 79.87\n",
      "\n",
      "üîπ Evaluating BERT (Fine-tuned News QA)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò Dataset: Colombia and Mexico Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3235/3235 [2:07:58<00:00,  2.37s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia and Mexico Train:\n",
      "{'exact': 88.35283651543146, 'f1': 92.23009713323718, 'total': 14613, 'HasAns_exact': 83.24638251796436, 'HasAns_f1': 88.82354655064495, 'HasAns_total': 10159, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 4454, 'best_exact': 88.35283651543146, 'best_exact_thresh': 0.953493058681488, 'best_f1': 92.23009713323636, 'best_f1_thresh': 0.9999802112579346}\n",
      "   ‚Ä¢ Exact Match (EM): 88.35\n",
      "   ‚Ä¢ F1 Score: 92.23\n",
      "\n",
      "üìò Dataset: Colombia and Mexico Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2845/2845 [53:02<00:00,  1.12s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia and Mexico Eval:\n",
      "{'exact': 78.42886795465432, 'f1': 84.37425737867055, 'total': 6263, 'HasAns_exact': 69.10587697233021, 'HasAns_f1': 77.62084929398881, 'HasAns_total': 4373, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 1890, 'best_exact': 78.42886795465432, 'best_exact_thresh': 0.9982497692108154, 'best_f1': 84.37425737867095, 'best_f1_thresh': 0.9999784231185913}\n",
      "   ‚Ä¢ Exact Match (EM): 78.43\n",
      "   ‚Ä¢ F1 Score: 84.37\n",
      "\n",
      "üìò Dataset: Colombia Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2564/2564 [1:23:17<00:00,  1.95s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia Train:\n",
      "{'exact': 88.39668476707631, 'f1': 92.220301347696, 'total': 10497, 'HasAns_exact': 83.21389195148842, 'HasAns_f1': 88.74538357866118, 'HasAns_total': 7256, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 3241, 'best_exact': 88.39668476707631, 'best_exact_thresh': 0.953493058681488, 'best_f1': 92.22030134769501, 'best_f1_thresh': 0.9999802112579346}\n",
      "   ‚Ä¢ Exact Match (EM): 88.40\n",
      "   ‚Ä¢ F1 Score: 92.22\n",
      "\n",
      "üìò Dataset: Mexico Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1006/1006 [45:16<00:00,  2.70s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Mexico Train:\n",
      "{'exact': 88.24101068999028, 'f1': 92.25507924228305, 'total': 4116, 'HasAns_exact': 83.3275921460558, 'HasAns_f1': 89.01891359326116, 'HasAns_total': 2903, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 1213, 'best_exact': 88.24101068999028, 'best_exact_thresh': 0.9047306776046753, 'best_f1': 92.25507924228287, 'best_f1_thresh': 0.9999642968177795}\n",
      "   ‚Ä¢ Exact Match (EM): 88.24\n",
      "   ‚Ä¢ F1 Score: 92.26\n",
      "\n",
      "üìò Dataset: Colombia Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2211/2211 [35:52<00:00,  1.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia Eval:\n",
      "{'exact': 78.01733718604135, 'f1': 83.97446106885951, 'total': 4499, 'HasAns_exact': 68.61313868613139, 'HasAns_f1': 77.11872432522999, 'HasAns_total': 3151, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 1348, 'best_exact': 78.01733718604135, 'best_exact_thresh': 0.9982497692108154, 'best_f1': 83.97446106885934, 'best_f1_thresh': 0.9999523162841797}\n",
      "   ‚Ä¢ Exact Match (EM): 78.02\n",
      "   ‚Ä¢ F1 Score: 83.97\n",
      "\n",
      "üìò Dataset: Mexico Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 880/880 [21:45<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Mexico Eval:\n",
      "{'exact': 79.47845804988663, 'f1': 85.3939192822075, 'total': 1764, 'HasAns_exact': 70.37643207855974, 'HasAns_f1': 78.91560852194272, 'HasAns_total': 1222, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 542, 'best_exact': 79.47845804988663, 'best_exact_thresh': 0.9959526062011719, 'best_f1': 85.39391928220763, 'best_f1_thresh': 0.9999784231185913}\n",
      "   ‚Ä¢ Exact Match (EM): 79.48\n",
      "   ‚Ä¢ F1 Score: 85.39\n",
      "\n",
      "üîπ Evaluating DistilBERT (Fine-tuned News QA)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò Dataset: Colombia and Mexico Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3235/3235 [2:12:25<00:00,  2.46s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia and Mexico Train:\n",
      "{'exact': 88.31177718469856, 'f1': 92.15736917269533, 'total': 14613, 'HasAns_exact': 83.18732158677035, 'HasAns_f1': 88.7189325446012, 'HasAns_total': 10159, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 4454, 'best_exact': 88.31177718469856, 'best_exact_thresh': 0.9998776316642761, 'best_f1': 92.15736917269462, 'best_f1_thresh': 0.9999001026153564}\n",
      "   ‚Ä¢ Exact Match (EM): 88.31\n",
      "   ‚Ä¢ F1 Score: 92.16\n",
      "\n",
      "üìò Dataset: Colombia and Mexico Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2845/2845 [53:08<00:00,  1.12s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia and Mexico Eval:\n",
      "{'exact': 78.62046942359892, 'f1': 84.59307474959367, 'total': 6263, 'HasAns_exact': 69.38028813171735, 'HasAns_f1': 77.93423900221907, 'HasAns_total': 4373, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 1890, 'best_exact': 78.62046942359892, 'best_exact_thresh': 0.9985507130622864, 'best_f1': 84.59307474959402, 'best_f1_thresh': 0.9998937249183655}\n",
      "   ‚Ä¢ Exact Match (EM): 78.62\n",
      "   ‚Ä¢ F1 Score: 84.59\n",
      "\n",
      "üìò Dataset: Colombia Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2564/2564 [1:19:40<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia Train:\n",
      "{'exact': 88.3014194531771, 'f1': 92.08420492157332, 'total': 10497, 'HasAns_exact': 83.0760749724366, 'HasAns_f1': 88.54849766562249, 'HasAns_total': 7256, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 3241, 'best_exact': 88.3014194531771, 'best_exact_thresh': 0.9998776316642761, 'best_f1': 92.0842049215725, 'best_f1_thresh': 0.9999001026153564}\n",
      "   ‚Ä¢ Exact Match (EM): 88.30\n",
      "   ‚Ä¢ F1 Score: 92.08\n",
      "\n",
      "üìò Dataset: Mexico Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1006/1006 [44:02<00:00,  2.63s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Mexico Train:\n",
      "{'exact': 88.33819241982508, 'f1': 92.34395934374338, 'total': 4116, 'HasAns_exact': 83.4653806407165, 'HasAns_f1': 89.14493167717815, 'HasAns_total': 2903, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 1213, 'best_exact': 88.33819241982508, 'best_exact_thresh': 0.9492854475975037, 'best_f1': 92.34395934374325, 'best_f1_thresh': 0.9998850226402283}\n",
      "   ‚Ä¢ Exact Match (EM): 88.34\n",
      "   ‚Ä¢ F1 Score: 92.34\n",
      "\n",
      "üìò Dataset: Colombia Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2211/2211 [35:06<00:00,  1.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Colombia Eval:\n",
      "{'exact': 78.0395643476328, 'f1': 84.12246541797725, 'total': 4499, 'HasAns_exact': 68.64487464297049, 'HasAns_f1': 77.33004503823568, 'HasAns_total': 3151, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 1348, 'best_exact': 78.0395643476328, 'best_exact_thresh': 0.9985507130622864, 'best_f1': 84.12246541797711, 'best_f1_thresh': 0.9998937249183655}\n",
      "   ‚Ä¢ Exact Match (EM): 78.04\n",
      "   ‚Ä¢ F1 Score: 84.12\n",
      "\n",
      "üìò Dataset: Mexico Eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 880/880 [19:13<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results for Mexico Eval:\n",
      "{'exact': 80.10204081632654, 'f1': 85.79334197348315, 'total': 1764, 'HasAns_exact': 71.27659574468085, 'HasAns_f1': 79.49218923177104, 'HasAns_total': 1222, 'NoAns_exact': 100.0, 'NoAns_f1': 100.0, 'NoAns_total': 542, 'best_exact': 80.10204081632654, 'best_exact_thresh': 0.9905273914337158, 'best_f1': 85.79334197348324, 'best_f1_thresh': 0.9998733401298523}\n",
      "   ‚Ä¢ Exact Match (EM): 80.10\n",
      "   ‚Ä¢ F1 Score: 85.79\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
