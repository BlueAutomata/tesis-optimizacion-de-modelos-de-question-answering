{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Entrenamiento con Simple Transformers del model Distill BERT",
   "id": "ab9a3f8c86c7fc6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 1: Cargar datos en Google Colab",
   "id": "d000103e1003500a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7cbab1c4f699f9be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 2: Instalar librer√≠as",
   "id": "3796c502613c3c16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install transformers evaluate torch --quiet\n",
    "!pip install simpletransformers transformers datasets huggingface_hub scikit-learn\n",
    "!pip install evaluate --quiet"
   ],
   "id": "68c85e634945e38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 3: Cargar librer√≠as",
   "id": "a4582e8e16ddc92a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import evaluate\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import files"
   ],
   "id": "2aa4cdd6a449d333"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)"
   ],
   "id": "71e259d71be43e62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ],
   "id": "2307bcc66ef03277"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 4: Cargar datos",
   "id": "d9634d65f3fc304f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# URLs of the files\n",
    "urls = {\n",
    "    \"eval_colombia_mexico_dataset.json\": \"https://github.com/BlueAutomata/tesis-optimizacion-de-modelos-de-question-answering/raw/refs/heads/master/src/datasets/exploration_datasets/gold/eval_colombia_mexico_dataset.json\",\n",
    "    \"train_colombia_mexico_dataset.json\": \"https://github.com/BlueAutomata/tesis-optimizacion-de-modelos-de-question-answering/raw/refs/heads/master/src/datasets/exploration_datasets/gold/train_colombia_mexico_dataset.json\"\n",
    "}\n",
    "\n",
    "# Dictionary to store the loaded JSON data\n",
    "datasets = {}\n",
    "\n",
    "for filename, url in urls.items():\n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Save locally\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        # Load JSON into Python\n",
    "        datasets[filename] = response.json()\n",
    "        print(f\"{filename} downloaded and loaded successfully!\")\n",
    "    else:\n",
    "        print(f\"Failed to download {filename}. Status code: {response.status_code}\")"
   ],
   "id": "d8367d5e054755ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(\"train_colombia_mexico_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_dataset = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(train_dataset)} records successfully!\")"
   ],
   "id": "c4550c2a21ed493f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(\"eval_colombia_mexico_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(eval_dataset)} records successfully!\")"
   ],
   "id": "d461a1fca8eacc28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_dataset = train_dataset[\"data\"]",
   "id": "f396a361c6d6a52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "eval_dataset = eval_dataset[\"data\"]",
   "id": "8f773c36e370721b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# üîß 1Ô∏è‚É£ Flatten your dataset so each row has 'context' and 'qas'\n",
    "def flatten_squad(dataset):\n",
    "    new_data = []\n",
    "    for article in dataset:\n",
    "        for para in article[\"paragraphs\"]:\n",
    "            new_data.append({\n",
    "                \"context\": para[\"context\"],\n",
    "                \"qas\": para[\"qas\"]\n",
    "            })\n",
    "    return new_data"
   ],
   "id": "ecb13996119c4376"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_data = flatten_squad(train_dataset)",
   "id": "172b65ac300f0a31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "eval_data = flatten_squad(eval_dataset)",
   "id": "9c6839060543a6d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"‚úÖ Training samples: {len(train_data)}\")\n",
    "print(f\"‚úÖ Eval samples: {len(eval_data)}\")"
   ],
   "id": "78cf28e633212ca9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 5: Definir hiperpar√°metros",
   "id": "a0b0a858ec4228b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_args = QuestionAnsweringArgs()\n",
    "\n",
    "# Training behavior\n",
    "model_args.train_batch_size = 8\n",
    "model_args.eval_batch_size = 8\n",
    "model_args.num_train_epochs = 2\n",
    "model_args.learning_rate = 5e-6\n",
    "model_args.gradient_accumulation_steps = 1\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_steps = 500\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.save_steps = -1\n",
    "model_args.best_model_dir = \"./outputs/best_model/\"\n",
    "model_args.output_dir = \"./outputs/\"\n",
    "\n",
    "# Optimization\n",
    "model_args.max_seq_length = 384\n",
    "model_args.doc_stride = 128\n",
    "model_args.warmup_ratio = 0.1\n",
    "model_args.max_answer_length = 30\n",
    "\n",
    "# Logging\n",
    "model_args.logging_steps = 100\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "model_args.manual_seed = 42\n",
    "\n",
    "# üîπ Sliding window parameters\n",
    "model_args.max_seq_length = 384          # maximum total input sequence length after tokenization\n",
    "model_args.doc_stride = 128              # overlap between two sliding windows\n",
    "model_args.max_query_length = 64         # maximum length of the question\n",
    "\n",
    "# Resource handling\n",
    "model_args.use_multiprocessing = False  # safer for notebooks\n",
    "model_args.fp16 = torch.cuda.is_available()  # use mixed precision if CUDA available"
   ],
   "id": "d44d1467b078e4c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 6: Cargar el modelo",
   "id": "c4d3d05d31c843bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_original = QuestionAnsweringModel(\n",
    "    model_type=\"bert\",\n",
    "    model_name=\"mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\",  # BETO\n",
    "    args=model_args,\n",
    "    use_cuda=torch.cuda.is_available()\n",
    ")"
   ],
   "id": "5d2dae3f0699f9cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = QuestionAnsweringModel(\n",
    "    model_type=\"bert\",\n",
    "    model_name=\"mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\",  # BETO\n",
    "    args=model_args,\n",
    "    use_cuda=torch.cuda.is_available()\n",
    ")"
   ],
   "id": "90aa0488a272e8e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 7: Entrenamiento del modelo",
   "id": "cb4a464b9e41b900"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.train_model(train_data, eval_data=eval_data)",
   "id": "d17f9a57b25b2da6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 8: Evaluaci√≥n de los resultados",
   "id": "33b45d953e9a4028"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "result_original, texts_original = model_original.eval_model(eval_data)\n",
    "print(\"üìä Evaluation results:\")\n",
    "print(result_original)"
   ],
   "id": "e51c2a79ca2f1f06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "result, texts = model.eval_model(eval_data)\n",
    "print(\"üìä Evaluation results:\")\n",
    "print(result)"
   ],
   "id": "66bf58120b3e3575"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "correct = result_original['correct']\n",
    "similar = result_original['similar']\n",
    "incorrect = result_original['incorrect']\n",
    "total = correct + similar + incorrect\n",
    "\n",
    "# 1Ô∏è‚É£ Exact Match Accuracy\n",
    "exact_match = correct / total\n",
    "\n",
    "# 2Ô∏è‚É£ Weighted Accuracy (partial credit for 'similar')\n",
    "weighted_accuracy = (correct + 0.5 * similar) / total\n",
    "\n",
    "# 3Ô∏è‚É£ F1 Score approximation\n",
    "TP = correct + 0.5 * similar\n",
    "FN = 0.5 * similar + incorrect\n",
    "# Assuming FP = 0 (as Simple Transformers counts predictions, not negatives)\n",
    "precision = TP / TP\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print results\n",
    "print(f\"Exact Match (EM): {exact_match:.4f} ‚Üí {exact_match*100:.2f}%\")\n",
    "print(f\"Weighted Accuracy: {weighted_accuracy:.4f} ‚Üí {weighted_accuracy*100:.2f}%\")\n",
    "print(f\"F1 Score: {f1_score:.4f} ‚Üí {f1_score*100:.2f}%\")"
   ],
   "id": "ce22d512658db91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "correct = result['correct']\n",
    "similar = result['similar']\n",
    "incorrect = result['incorrect']\n",
    "total = correct + similar + incorrect\n",
    "\n",
    "# 1Ô∏è‚É£ Exact Match Accuracy\n",
    "exact_match = correct / total\n",
    "\n",
    "# 2Ô∏è‚É£ Weighted Accuracy (partial credit for 'similar')\n",
    "weighted_accuracy = (correct + 0.5 * similar) / total\n",
    "\n",
    "# 3Ô∏è‚É£ F1 Score approximation\n",
    "TP = correct + 0.5 * similar\n",
    "FN = 0.5 * similar + incorrect\n",
    "# Assuming FP = 0 (as Simple Transformers counts predictions, not negatives)\n",
    "precision = TP / TP\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print results\n",
    "print(f\"Exact Match (EM): {exact_match:.4f} ‚Üí {exact_match*100:.2f}%\")\n",
    "print(f\"Weighted Accuracy: {weighted_accuracy:.4f} ‚Üí {weighted_accuracy*100:.2f}%\")\n",
    "print(f\"F1 Score: {f1_score:.4f} ‚Üí {f1_score*100:.2f}%\")"
   ],
   "id": "e2d76879f5d8947e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Paso 9: Guardar los resultados",
   "id": "d90721370c670ad6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Folder to save\n",
    "local_path = \"./QA_model_bert\"\n",
    "os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "# Save the Hugging Face model & tokenizer directly\n",
    "model.model.save_pretrained(local_path)       # Saves weights + config\n",
    "model.tokenizer.save_pretrained(local_path)   # Saves vocab + tokenizer config\n",
    "\n",
    "# Check files\n",
    "!ls -l ./QA_model"
   ],
   "id": "febfd73b8df157d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "shutil.make_archive(\"QA_model_bert\", 'zip', local_path)\n",
    "print(\"‚úÖ Zipped model\")\n",
    "!ls -lh QA_model.zip"
   ],
   "id": "4f500b66f69085bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "files.download(\"QA_model_bert.zip\")",
   "id": "2c92d2013f50f3fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "id": "d863157018ee32bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# This will open a prompt for your Hugging Face token\n",
    "login()"
   ],
   "id": "be76166f5e8a056"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from huggingface_hub import whoami\n",
    "print(whoami())"
   ],
   "id": "1470a4ccca9a83dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from huggingface_hub import login, create_repo\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "# 1Ô∏è‚É£ Define your paths and repo name\n",
    "model_dir = \"./outputs\"\n",
    "repo_id = \"BlueAutomata/distill-bert-base-spanish-wwm-cased-news-qa-colombia-mexico\"\n",
    "\n",
    "# 2Ô∏è‚É£ Create the repo (won‚Äôt fail if it already exists)\n",
    "create_repo(repo_id, private=False, exist_ok=True)\n",
    "\n",
    "# 3Ô∏è‚É£ Load your SimpleTransformers model as a standard HF model\n",
    "hf_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# 4Ô∏è‚É£ Push to the Hugging Face Hub\n",
    "hf_model.push_to_hub(\n",
    "    repo_id,\n",
    "    description=\"Distill-BERT-base Spanish WWM cased model fine-tuned for extractive QA on news articles from Colombia and Mexico.\",\n",
    "    tags=[\"spanish\", \"qa\", \"news\", \"colombia\", \"mexico\", \"bert-base\", \"wwm\", \"cased\"]\n",
    ")\n",
    "\n",
    "hf_tokenizer.push_to_hub(repo_id)"
   ],
   "id": "2040118b2d577eac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "shutil.move(\"QA_model_bert.zip\", \"/content/drive/MyDrive/Tahesis_QA_Optimization/Model\")"
   ],
   "id": "cf7868adf6d7eb4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Path to the folder containing the saved model\n",
    "model_path = \"./QA_model_bert\"  # change if different\n",
    "\n",
    "# Reload the model\n",
    "my_model = QuestionAnsweringModel(\n",
    "    \"bert\",\n",
    "    model_path,\n",
    "    use_cuda=True  # set to False if no GPU\n",
    ")"
   ],
   "id": "ad8d8e27d7b8f91b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Context & question\n",
    "context = \"Ciudad de M√©xico. El capit√°n de la Secretar√≠a de Marina, Abraham Jerem√≠as P√©rez Ram√≠rez, fue hallado muerto en Tamaulipas.\"\n",
    "question = \"¬øQui√©n fue hallado muerto en Tamaulipas?\"\n",
    "\n",
    "# Prepare input in SimpleTransformers format\n",
    "to_predict = [\n",
    "    {\n",
    "        \"context\": context,\n",
    "        \"qas\": [\n",
    "            {\n",
    "                \"id\": \"0\",\n",
    "                \"question\": question,\n",
    "                \"answers\": [{\"text\": \" \", \"answer_start\": 0}],\n",
    "                \"is_impossible\": False\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run prediction\n",
    "answers = my_model.predict(to_predict)\n",
    "print(answers)"
   ],
   "id": "e04df4b67b356b98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load your fine-tuned model from the Hub\n",
    "qa = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"BlueAutomata/bert-base-spanish-wwm-cased-news-qa-colombia-mexico\",\n",
    "    tokenizer=\"BlueAutomata/bert-base-spanish-wwm-cased-news-qa-colombia-mexico\"\n",
    ")\n",
    "\n",
    "# Provide Spanish context\n",
    "contexto = \"\"\"\n",
    "El presidente Gustavo Petro anunci√≥ nuevas medidas para impulsar el uso de energ√≠as renovables en Colombia,\n",
    "especialmente en la regi√≥n del Caribe, donde los proyectos solares y e√≥licos han ganado protagonismo.\n",
    "El objetivo del gobierno es reducir las emisiones de carbono en un 30% para el a√±o 2030.\n",
    "\"\"\"\n",
    "\n",
    "# Ask questions in Spanish\n",
    "preguntas = [\n",
    "    \"¬øQui√©n anunci√≥ nuevas medidas para energ√≠as renovables?\",\n",
    "    \"¬øEn qu√© regi√≥n se impulsar√°n los proyectos solares y e√≥licos?\",\n",
    "    \"¬øCu√°l es el objetivo del gobierno para 2030?\"\n",
    "]\n",
    "\n",
    "# Evaluate each question\n",
    "for pregunta in preguntas:\n",
    "    respuesta = qa(question=pregunta, context=contexto)\n",
    "    print(f\"‚ùì {pregunta}\\nüí¨ {respuesta['answer']}\\n\")"
   ],
   "id": "a126aa2dd4e39a45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def flatten_squad(dataset):\n",
    "    # If the dataset is a dict with \"data\", extract it\n",
    "    if isinstance(dataset, dict) and \"data\" in dataset:\n",
    "        dataset = dataset[\"data\"]\n",
    "\n",
    "    new_data = []\n",
    "    for article in dataset:\n",
    "        for para in article[\"paragraphs\"]:\n",
    "            new_data.append({\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"context\": para[\"context\"],\n",
    "                \"qas\": para[\"qas\"]\n",
    "            })\n",
    "    return new_data"
   ],
   "id": "29c0429737ef0cb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "flat_eval = flatten_squad(eval_dataset)",
   "id": "fae3dfd2bed29f0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"BlueAutomata/bert-base-spanish-wwm-cased-news-qa-colombia-mexico\",\n",
    "    tokenizer=\"BlueAutomata/bert-base-spanish-wwm-cased-news-qa-colombia-mexico\"\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"squad\")"
   ],
   "id": "a3879adb42550a52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "\n",
    "qa_pipeline_original = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\",\n",
    "    tokenizer=\"mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es\"\n",
    ")\n"
   ],
   "id": "180c7a5f34ee6f78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "predictions_original = []\n",
    "references_original = []\n",
    "\n",
    "for ex in flat_eval:\n",
    "    context = ex[\"context\"]\n",
    "    for qa in ex[\"qas\"]:\n",
    "        if qa[\"is_impossible\"]:\n",
    "            continue  # Skip unanswerable questions\n",
    "\n",
    "        # Run QA prediction\n",
    "        pred = qa_pipeline_original(question=qa[\"question\"], context=context)\n",
    "\n",
    "        # Collect prediction and reference\n",
    "        predictions_original.append({\n",
    "            \"id\": qa[\"id\"],\n",
    "            \"prediction_text\": pred[\"answer\"]\n",
    "        })\n",
    "\n",
    "        references_original.append({\n",
    "            \"id\": qa[\"id\"],\n",
    "            \"answers\": {\n",
    "                \"text\": [a[\"text\"] for a in qa[\"answers\"]],\n",
    "                \"answer_start\": [a[\"answer_start\"] for a in qa[\"answers\"]]\n",
    "            }\n",
    "        })"
   ],
   "id": "c722fb241e2f30b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for ex in flat_eval:\n",
    "    context = ex[\"context\"]\n",
    "    for qa in ex[\"qas\"]:\n",
    "        if qa[\"is_impossible\"]:\n",
    "            continue  # Skip unanswerable questions\n",
    "\n",
    "        # Run QA prediction\n",
    "        pred = qa_pipeline(question=qa[\"question\"], context=context)\n",
    "\n",
    "        # Collect prediction and reference\n",
    "        predictions.append({\n",
    "            \"id\": qa[\"id\"],\n",
    "            \"prediction_text\": pred[\"answer\"]\n",
    "        })\n",
    "\n",
    "        references.append({\n",
    "            \"id\": qa[\"id\"],\n",
    "            \"answers\": {\n",
    "                \"text\": [a[\"text\"] for a in qa[\"answers\"]],\n",
    "                \"answer_start\": [a[\"answer_start\"] for a in qa[\"answers\"]]\n",
    "            }\n",
    "        })\n"
   ],
   "id": "bfa61ddd7c1717b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = metric.compute(predictions=predictions_original, references=references_original)\n",
    "print(\"üìä Evaluation results:\")\n",
    "print(f\"Exact Match: {results['exact_match']:.2f}\")\n",
    "print(f\"F1 Score: {results['f1']:.2f}\")"
   ],
   "id": "bdffc17f1997c966"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(\"üìä Evaluation results:\")\n",
    "print(f\"Exact Match: {results['exact_match']:.2f}\")\n",
    "print(f\"F1 Score: {results['f1']:.2f}\")"
   ],
   "id": "4f84b35f4262f7ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"squad_v2\")"
   ],
   "id": "7f9c44fd7ba71c31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions_original = []\n",
    "references_original = []\n",
    "\n",
    "for ex in tqdm(flat_eval):\n",
    "    for qa in ex[\"qas\"]:\n",
    "        if qa[\"is_impossible\"]:\n",
    "            # Questions that have no valid answer in the text\n",
    "            predictions_original.append({\n",
    "                \"id\": qa[\"id\"],\n",
    "                \"prediction_text\": \"\",\n",
    "                \"no_answer_probability\": 1.0   # fully confident it's unanswerable\n",
    "            })\n",
    "            references_original.append({\n",
    "                \"id\": qa[\"id\"],\n",
    "                \"answers\": {\"text\": [], \"answer_start\": []}\n",
    "            })\n",
    "        else:\n",
    "            # Normal (answerable) questions\n",
    "            pred = qa_pipeline_original(question=qa[\"question\"], context=ex[\"context\"])\n",
    "\n",
    "            predictions_original.append({\n",
    "                \"id\": qa[\"id\"],\n",
    "                \"prediction_text\": pred[\"answer\"],\n",
    "                # Use model confidence inversely as no-answer probability\n",
    "                \"no_answer_probability\": 1.0 - pred.get(\"score\", 0.0)\n",
    "            })\n",
    "\n",
    "            references_original.append({\n",
    "                \"id\": qa[\"id\"],\n",
    "                \"answers\": {\n",
    "                    \"text\": [a[\"text\"] for a in qa[\"answers\"]],\n",
    "                    \"answer_start\": [a[\"answer_start\"] for a in qa[\"answers\"]]\n",
    "                }\n",
    "            })"
   ],
   "id": "dc372be1b31deaa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for ex in tqdm(flat_eval):\n",
    "    for qa in ex[\"qas\"]:\n",
    "        if qa[\"is_impossible\"]:\n",
    "            # Questions that have no valid answer in the text\n",
    "            predictions.append({\n",
    "                \"id\": qa[\"id\"],\n",
    "                \"prediction_text\": \"\",\n",
    "                \"no_answer_probability\": 1.0   # fully confident it's unanswerable\n",
    "            })\n",
    "            references.append({\n",
    "                \"id\": qa[\"id\"],\n",
    "                \"answers\": {\"text\": [], \"answer_start\": []}\n",
    "            })\n",
    "        else:\n",
    "            # Normal (answerable) questions\n",
    "            pred = qa_pipeline(question=qa[\"question\"], context=ex[\"context\"])\n",
    "\n",
    "            predictions.append({\n",
    "                \"id\": qa[\"id\"],\n",
    "                \"prediction_text\": pred[\"answer\"],\n",
    "                # Use model confidence inversely as no-answer probability\n",
    "                \"no_answer_probability\": 1.0 - pred.get(\"score\", 0.0)\n",
    "            })\n",
    "\n",
    "            references.append({\n",
    "                \"id\": qa[\"id\"],\n",
    "                \"answers\": {\n",
    "                    \"text\": [a[\"text\"] for a in qa[\"answers\"]],\n",
    "                    \"answer_start\": [a[\"answer_start\"] for a in qa[\"answers\"]]\n",
    "                }\n",
    "            })"
   ],
   "id": "b229074ad194dd4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "predictions_original",
   "id": "f60a99fb0a146610"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = metric.compute(predictions=predictions_original, references=references_original)\n",
    "\n",
    "print(\"üìä Evaluation results:\")\n",
    "print(f\"Exact Match: {results['exact']:.2f}\")\n",
    "print(f\"F1 Score: {results['f1']:.2f}\")"
   ],
   "id": "a8b92b4c2a518fe0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(\"üìä Evaluation results:\")\n",
    "print(f\"Exact Match: {results['exact']:.2f}\")\n",
    "print(f\"F1 Score: {results['f1']:.2f}\")"
   ],
   "id": "95c9d563d79d2518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!mv outputs outputs_bert\n",
    "!zip -r outputs_bert.zip outputs_bert"
   ],
   "id": "3891df52baea7d10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "shutil.move(\"outputs_bert.zip\", \"/content/drive/MyDrive/Thesis_QA_Optimization/Model\")",
   "id": "29c52be2b59dabcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the full path to the destination directory\n",
    "destination_dir = '/content/drive/MyDrive/Thesis_QA_Optimization/Model'\n",
    "source_file = 'outputs_bert.zip' # This is the file you want to move\n",
    "\n",
    "# 1. Check if the directory exists and create it if it doesn't\n",
    "# The `exist_ok=True` argument prevents an error if the directory already exists.\n",
    "# The `os.makedirs` function creates all intermediate-level directories needed.\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# 2. Now you can safely move the file\n",
    "try:\n",
    "    shutil.move(source_file, destination_dir)\n",
    "    print(f\"Successfully moved {source_file} to {destination_dir}\")\n",
    "except FileNotFoundError as e:\n",
    "    # This might catch a different FileNotFoundError if the source file doesn't exist,\n",
    "    # but the primary directory issue should be resolved by os.makedirs.\n",
    "    print(f\"Error moving file: {e}\")"
   ],
   "id": "f24bf751d503110b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
